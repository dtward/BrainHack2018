{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDDMM in tensorflow\n",
    "There are two computationally demanding challenges in Tensorflow.  Interpolation, and fft.  In tensorflow we will have to implement linear interpolation based on linear combinations of slicing.\n",
    "\n",
    "I will start with a 2d implementation, then go to 3d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing stuff\n",
    "I'll start by importing libraries and setting up\n",
    "\n",
    "Standard numerics, reading neuroimages, and tensorflow, finding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion() # display during, note I can call fig.canvas.draw()\n",
    "import tensorflow as tf\n",
    "import nibabel as nib\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting images\n",
    "Load some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('*MNI.img')\n",
    "img0 = nib.load(files[0])\n",
    "img1 = nib.load(files[2])\n",
    "# some info\n",
    "#print(dir(img0))\n",
    "#print(img0.header)\n",
    "nx = img0.header['dim'][1:4]\n",
    "#print(nx)\n",
    "\n",
    "dx = img0.header['pixdim'][1:4]\n",
    "#print(dx)\n",
    "\n",
    "x0 = np.arange(0,nx[0])*dx[0]\n",
    "x1 = np.arange(0,nx[1])*dx[1]\n",
    "x2 = np.arange(0,nx[2])*dx[2]\n",
    "x0 = x0 - np.mean(x0)\n",
    "x1 = x1 - np.mean(x1)\n",
    "x2 = x2 - np.mean(x2)\n",
    "\n",
    "#print(img0.shape)\n",
    "\n",
    "# a picture\n",
    "f,ax = plt.subplots(1,2,squeeze=True,sharex=True,sharey=True)\n",
    "s = int(img0.shape[2]/2*0.96)\n",
    "imshowargs = {'aspect':'equal', 'cmap':'gray', 'interpolation':'none'}\n",
    "ax[0].imshow(np.squeeze(img0.get_data()[:,:,s]), **imshowargs)\n",
    "ax[1].imshow(np.squeeze(img1.get_data()[:,:,s]), **imshowargs)\n",
    "\n",
    "_ = [ (a.set_xlabel('x (mm)'), a.set_ylabel('y (mm)'), a.set_title('Brain {}'.format(i))) for i,a in enumerate(ax)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some 2D images\n",
    "Get 2D images for my first test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = img0.get_data()[:,:,s,0]\n",
    "J = img1.get_data()[:,:,s,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an example deformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0,X1 = np.meshgrid(x0,x1,indexing='ij')\n",
    "# identity transformation\n",
    "id0 = X0\n",
    "id1 = X1\n",
    "# nonlinear\n",
    "scale = 20.0\n",
    "phi0 = id0 + np.exp(-(X0**2 + X1**2)/2.0/scale**2)*20\n",
    "phi1 = id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Itf = tf.get_variable('Itf',initializer=tf.constant_initializer(I),shape=nx[:2],\n",
    "                             dtype=tf.float64, trainable=False)\n",
    "Jtf = tf.get_variable('Jtf',initializer=tf.constant_initializer(J),shape=nx[:2],\n",
    "                             dtype=tf.float64, trainable=False)\n",
    "phi0tf = tf.get_variable('phi0tf',initializer=tf.constant_initializer(phi0),shape=nx[:2],\n",
    "                         dtype=tf.float64, trainable=False)\n",
    "phi1tf = tf.get_variable('phi1tf',initializer=tf.constant_initializer(phi1),shape=nx[:2],\n",
    "                         dtype=tf.float64, trainable=False)\n",
    "id0tf = tf.get_variable('id0tf',initializer=tf.constant_initializer(X0),shape=nx[:2],\n",
    "                       dtype=tf.float64, trainable=False)\n",
    "id1tf = tf.get_variable('id1tf',initializer=tf.constant_initializer(X1),shape=nx[:2],\n",
    "                       dtype=tf.float64, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now we need to start adding the operations\n",
    "The first thing we will need to do is convert the transformations to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to index\n",
    "phi0_index = (phi0tf - x0[0])/dx[0]\n",
    "phi1_index = (phi1tf - x1[0])/dx[2]\n",
    "# take the floor to get integers\n",
    "phi0_index_floor = tf.floor(phi0_index)\n",
    "phi1_index_floor = tf.floor(phi1_index)\n",
    "# get the fraction to the next pixel\n",
    "phi0_p = phi0_index - phi0_index_floor\n",
    "phi1_p = phi1_index - phi1_index_floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then we need to get the next samples\n",
    "phi0_index_floor_1 = phi0_index_floor+1\n",
    "phi1_index_floor_1 = phi1_index_floor+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and apply boundary conditions\n",
    "phi0_index_floor = tf.minimum(phi0_index_floor,nx[0]-1)\n",
    "phi0_index_floor = tf.maximum(phi0_index_floor,0)\n",
    "\n",
    "phi0_index_floor_1 = tf.minimum(phi0_index_floor_1,nx[0]-1)\n",
    "phi0_index_floor_1 = tf.maximum(phi0_index_floor_1,0)\n",
    "\n",
    "phi1_index_floor = tf.minimum(phi1_index_floor,nx[1]-1)\n",
    "phi1_index_floor = tf.maximum(phi1_index_floor,0)\n",
    "\n",
    "phi1_index_floor_1 = tf.minimum(phi1_index_floor_1,nx[1]-1)\n",
    "phi1_index_floor_1 = tf.maximum(phi1_index_floor_1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we will need to vectorize everything to use scalar indices\n",
    "phi0_index_floor_flat = tf.reshape(phi0_index_floor,[-1])\n",
    "phi0_index_floor_flat_1 = tf.reshape(phi0_index_floor_1,[-1])\n",
    "\n",
    "phi1_index_floor_flat = tf.reshape(phi1_index_floor,[-1])\n",
    "phi1_index_floor_flat_1 = tf.reshape(phi1_index_floor_1,[-1])\n",
    "\n",
    "I_flat = tf.reshape(Itf,[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will combine into a scalar index\n",
    "#phi_index_floor_flat = phi0_index_floor_flat + nx[0]*phi1_index_floor_flat\n",
    "# the above does not work\n",
    "# the below does work, but its odd\n",
    "# okay the answer is simply that the LAST INDEX IS CONTIGUOUS\n",
    "phi_index_floor_flat_00 = nx[1]*phi0_index_floor_flat + phi1_index_floor_flat\n",
    "phi_index_floor_flat_01 = nx[1]*phi0_index_floor_flat + phi1_index_floor_flat_1\n",
    "phi_index_floor_flat_10 = nx[1]*phi0_index_floor_flat_1 + phi1_index_floor_flat\n",
    "phi_index_floor_flat_11 = nx[1]*phi0_index_floor_flat_1 + phi1_index_floor_flat_1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now slice the image\n",
    "I00_flat = tf.gather(I_flat, tf.cast(phi_index_floor_flat_00, dtype=tf.int64))\n",
    "I01_flat = tf.gather(I_flat, tf.cast(phi_index_floor_flat_01, dtype=tf.int64))\n",
    "I10_flat = tf.gather(I_flat, tf.cast(phi_index_floor_flat_10, dtype=tf.int64))\n",
    "I11_flat = tf.gather(I_flat, tf.cast(phi_index_floor_flat_11, dtype=tf.int64))\n",
    "# reshape it\n",
    "I00 = tf.reshape(I00_flat,nx[:2])\n",
    "I01 = tf.reshape(I01_flat,nx[:2])\n",
    "I10 = tf.reshape(I10_flat,nx[:2])\n",
    "I11 = tf.reshape(I11_flat,nx[:2])\n",
    "\n",
    "# combine them!\n",
    "Il = I00*(1.0-phi0_p)*(1.0-phi1_p) \\\n",
    "    + I01*(1.0-phi0_p)*(    phi1_p) \\\n",
    "    + I10*(    phi0_p)*(1.0-phi1_p) \\\n",
    "    + I11*(    phi0_p)*(    phi1_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now start a session, and get I00\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #tf.initialize_variables([Itf,phi0tf,phi1tf])\n",
    "    I00np = I00.eval()\n",
    "    phishow = phi0_index_floor.eval()\n",
    "    Ilnp = Il.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2)\n",
    "ax[0].imshow(I00np,cmap='gray',aspect='equal')\n",
    "ax[1].imshow(Ilnp,cmap='gray',aspect='equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrapping into a nice function\n",
    "I will need to wrap this up into a nice function in order to use it meaningfully\n",
    "\n",
    "The inputs will \n",
    "\n",
    "One issue is if I want to deform a bunch of things with the same deformation, can I do it more efficiently?\n",
    "\n",
    "There should be a way to use gather like that on the last axis of a tensor or the first axis or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp2(x0,x1,Itf,phi0tf,phi1tf):\n",
    "    ''' \n",
    "    inputs x0, x1 should be numpy arrays\n",
    "    the rest should be tensorflow tensors\n",
    "    '''\n",
    "    # get the size\n",
    "    dx = [x0[1]-x0[0], x1[1]-x1[0]]\n",
    "    nx = [len(x0), len(x1)]    \n",
    "    #convert to index\n",
    "    phi0_index = (phi0tf - x0[0])/dx[0]\n",
    "    phi1_index = (phi1tf - x1[0])/dx[1]\n",
    "    # take the floor to get integers\n",
    "    phi0_index_floor = tf.floor(phi0_index)\n",
    "    phi1_index_floor = tf.floor(phi1_index)\n",
    "    # get the fraction to the next pixel\n",
    "    phi0_p = phi0_index - phi0_index_floor\n",
    "    phi1_p = phi1_index - phi1_index_floor\n",
    "    # get the next samples\n",
    "    phi0_index_floor_1 = phi0_index_floor+1\n",
    "    phi1_index_floor_1 = phi1_index_floor+1\n",
    "    # and apply boundary conditions\n",
    "    phi0_index_floor = tf.minimum(phi0_index_floor,nx[0]-1)\n",
    "    phi0_index_floor = tf.maximum(phi0_index_floor,0)\n",
    "    phi0_index_floor_1 = tf.minimum(phi0_index_floor_1,nx[0]-1)\n",
    "    phi0_index_floor_1 = tf.maximum(phi0_index_floor_1,0)\n",
    "    phi1_index_floor = tf.minimum(phi1_index_floor,nx[1]-1)\n",
    "    phi1_index_floor = tf.maximum(phi1_index_floor,0)\n",
    "    phi1_index_floor_1 = tf.minimum(phi1_index_floor_1,nx[1]-1)\n",
    "    phi1_index_floor_1 = tf.maximum(phi1_index_floor_1,0)\n",
    "    # then we will need to vectorize everything to use scalar indices\n",
    "    phi0_index_floor_flat = tf.reshape(phi0_index_floor,[-1])\n",
    "    phi0_index_floor_flat_1 = tf.reshape(phi0_index_floor_1,[-1])\n",
    "    phi1_index_floor_flat = tf.reshape(phi1_index_floor,[-1])\n",
    "    phi1_index_floor_flat_1 = tf.reshape(phi1_index_floor_1,[-1])\n",
    "    I_flat = tf.reshape(Itf,[-1])\n",
    "    # indices recall that the LAST INDEX IS CONTIGUOUS\n",
    "    phi_index_floor_flat_00 = nx[1]*phi0_index_floor_flat + phi1_index_floor_flat\n",
    "    phi_index_floor_flat_01 = nx[1]*phi0_index_floor_flat + phi1_index_floor_flat_1\n",
    "    phi_index_floor_flat_10 = nx[1]*phi0_index_floor_flat_1 + phi1_index_floor_flat\n",
    "    phi_index_floor_flat_11 = nx[1]*phi0_index_floor_flat_1 + phi1_index_floor_flat_1\n",
    "    # now slice the image\n",
    "    I00_flat = tf.gather(I_flat, tf.cast(phi_index_floor_flat_00, dtype=tf.int64))\n",
    "    I01_flat = tf.gather(I_flat, tf.cast(phi_index_floor_flat_01, dtype=tf.int64))\n",
    "    I10_flat = tf.gather(I_flat, tf.cast(phi_index_floor_flat_10, dtype=tf.int64))\n",
    "    I11_flat = tf.gather(I_flat, tf.cast(phi_index_floor_flat_11, dtype=tf.int64))\n",
    "    # reshape it\n",
    "    I00 = tf.reshape(I00_flat,nx[:2])\n",
    "    I01 = tf.reshape(I01_flat,nx[:2])\n",
    "    I10 = tf.reshape(I10_flat,nx[:2])\n",
    "    I11 = tf.reshape(I11_flat,nx[:2])\n",
    "    # combine them!\n",
    "    Il = I00*(1.0-phi0_p)*(1.0-phi1_p) \\\n",
    "        + I01*(1.0-phi0_p)*(    phi1_p) \\\n",
    "        + I10*(    phi0_p)*(1.0-phi1_p) \\\n",
    "        + I11*(    phi0_p)*(    phi1_p)\n",
    "    return Il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Il = interp2(x0,x1,Itf,phi0tf,phi1tf)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    Ilnp = Il.eval()\n",
    "f,ax = plt.subplots(1,2,sharex=True,sharey=True)\n",
    "ax[0].imshow(I,cmap='gray',aspect='equal',interpolation='none')\n",
    "ax[1].imshow(Ilnp,cmap='gray',aspect='equal',interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also will need gradients\n",
    "def grad2d(Itf,dx):\n",
    "    I_0 = (tf.manip.roll(Itf,shift=-1,axis=0) - tf.manip.roll(Itf,shift=1,axis=0))/2.0/dx[0]\n",
    "    I_1 = (tf.manip.roll(Itf,shift=-1,axis=1) - tf.manip.roll(Itf,shift=1,axis=1))/2.0/dx[1]\n",
    "    #out[0,:] = out[1,:]-out[0,:] # this doesn't work in tensorflow\n",
    "    # generally you cannot assign to a tensor\n",
    "    return I_0, I_1\n",
    "I_0,I_1 = grad2d(Itf,dx)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    I_0np = I_0.eval()\n",
    "    I_1np = I_1.eval()\n",
    "plt.figure()\n",
    "plt.imshow(np.sqrt(I_0np**2 + I_1np**2),cmap='gray',aspect='equal',interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last thing to test is ffts\n",
    "alpha = 2.0*dx[0]\n",
    "power = 2\n",
    "f0 = np.arange(nx[0])/dx[0]/nx[0]\n",
    "f1 = np.arange(nx[1])/dx[1]/nx[1]\n",
    "F0,F1 = np.meshgrid(f0, f1, indexing='ij')\n",
    "# smoothiing operator\n",
    "# identity minus laplacian, in fourier domain\n",
    "# AI[i,j] = I[i,j] - alpha^2( (I[i+1,j] - 2I[i,j] + I[i-1,j])/dx^2 + (I[i,j+1] - 2I[i,j] + I[i,j-1])/dy^2  )\n",
    "Lhat = (1.0 - alpha**2*((-2.0 + 2.0*np.cos(2*np.pi*dx[0]*F0))/dx[0]**2 \n",
    "    + (-2.0 + 2.0*np.cos(2*np.pi*dx[1]*F1))/dx[1]**2   ))**power\n",
    "# for real ffts we only half of this, TODO\n",
    "LLhat = Lhat**2\n",
    "Khat = 1.0/LLhat\n",
    "Khattf = tf.constant(Khat,dtype=tf.float64)\n",
    "LLhattf = tf.constant(LLhat,dtype=tf.float64)\n",
    "\n",
    "Ihat = tf.fft2d(tf.cast(Itf,tf.complex64))\n",
    "Ibhat = Ihat*Khat\n",
    "Ib = tf.real(tf.ifft2d(Ibhat))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    Ibnp = Ib.eval()\n",
    "plt.figure()\n",
    "plt.imshow(Ibnp,cmap='gray',aspect='equal',interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we're ready to implement LDDMM\n",
    "So far the only thing I'm not satisfied with is the gradient\n",
    "\n",
    "I will code one step, then I can run it\n",
    "\n",
    "see pde example for nice idea https://www.tensorflow.org/tutorials/pdes\n",
    "\n",
    "the graph will have to take v and compute vnew\n",
    "\n",
    "then we'll assign vnew to v, and repeat the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "# these should probably be placeholders\n",
    "nT = 5\n",
    "dt = 1.0/nT\n",
    "niter = 10\n",
    "epsilon = 1e-1\n",
    "sigmaM = 1e1\n",
    "sigmaR = 1e3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize some variables\n",
    "# the v's are the only ones I really need to keep\n",
    "vt0 = tf.get_variable('vt0',shape=[nx[0],nx[1],nT],dtype=tf.float64,trainable=False,initializer=tf.zeros_initializer())\n",
    "vt1 = tf.get_variable('vt1',shape=[nx[0],nx[1],nT],dtype=tf.float64,trainable=False,initializer=tf.zeros_initializer())\n",
    "vt0new = tf.get_variable('vt0new',shape=[nx[0],nx[1],nT],dtype=tf.float64,trainable=False,initializer=tf.zeros_initializer())\n",
    "vt1new = tf.get_variable('vt1new',shape=[nx[0],nx[1],nT],dtype=tf.float64,trainable=False,initializer=tf.zeros_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow forwards\n",
    "It = [Itf]\n",
    "phiinv0 = id0tf\n",
    "phiinv1 = id1tf\n",
    "for t in range(nT):\n",
    "    v0 = vt0[:,:,t]\n",
    "    v1 = vt1[:,:,t]\n",
    "    X0s = id0 - v0*dt\n",
    "    X1s = id1 - v1*dt\n",
    "    \n",
    "    # update diffeomorphism with nice boundary conditions\n",
    "    phiinv0 = interp2(x0,x1,phiinv0-id0,X0s,X1s)+X0s\n",
    "    phiinv1 = interp2(x0,x1,phiinv1-id1,X0s,X1s)+X1s\n",
    "    \n",
    "    # deform the image\n",
    "    It.append(interp2(x0,x1,Itf,phiinv0,phiinv1))\n",
    "    \n",
    "    # get regularization energy\n",
    "    # to do\n",
    "    \n",
    "# now compute the error\n",
    "lambda1 = (It[-1] - J)/tf.cast(sigmaM**2,tf.float64)\n",
    "\n",
    "# get matching energy \n",
    "# to do\n",
    "\n",
    "# flow the error backwards\n",
    "phiinv0 = id0tf\n",
    "phiinv1 = id1tf\n",
    "vt0new_ = []\n",
    "vt1new_ = []\n",
    "for t in range(nT-1,-1,-1):\n",
    "    v0 = vt0[:,:,t]\n",
    "    v1 = vt1[:,:,t]\n",
    "    X0s = id0 + v0*dt\n",
    "    X1s = id1 + v1*dt\n",
    "    phiinv0 = interp2(x0,x1,phiinv0-id0,X0s,X1s)+X0s\n",
    "    phiinv1 = interp2(x0,x1,phiinv1-id1,X0s,X1s)+X1s\n",
    "    \n",
    "    # compute the gradient of the image at this time\n",
    "    I_0,I_1 = grad2d(It[t],dx)\n",
    "    \n",
    "    # compute the determinanat of jacobian\n",
    "    phiinv0_0,phiinv0_1 = grad2d(phiinv0,dx)\n",
    "    phiinv1_0,phiinv1_1 = grad2d(phiinv1,dx)\n",
    "    detjac = phiinv0_0*phiinv1_1 - phiinv0_1*phiinv1_0\n",
    "    \n",
    "    # get the lambda for this time\n",
    "    lambda_ = interp2(x0,x1,lambda1,phiinv0,phiinv1)*detjac\n",
    "    \n",
    "    # set up the gradient\n",
    "    grad0 = -lambda_*I_0\n",
    "    grad1 = -lambda_*I_1\n",
    "    \n",
    "    # blur it\n",
    "    grad0hat = tf.fft2d(tf.cast(grad0,tf.complex64))\n",
    "    grad1hat = tf.fft2d(tf.cast(grad1,tf.complex64))\n",
    "    grad0hat = grad0hat*tf.cast(Khattf,tf.complex64)\n",
    "    grad1hat = grad1hat*tf.cast(Khattf,tf.complex64)\n",
    "    grad0 = tf.real(tf.ifft2d(grad0hat))\n",
    "    grad1 = tf.real(tf.ifft2d(grad1hat))\n",
    "    # for whatever reason it became 32 bit\n",
    "    grad0 = tf.cast(grad0,tf.float64)\n",
    "    grad1 = tf.cast(grad1,tf.float64)\n",
    "    \n",
    "    # add the regularization\n",
    "    grad0 = grad0 + v0\n",
    "    grad1 = grad1 + v1\n",
    "    \n",
    "    # and calculate the new v\n",
    "    vt0new_.append(v0 - epsilon*grad0)\n",
    "    vt1new_.append(v1 - epsilon*grad1)\n",
    "    \n",
    "vt0new_ = vt0new_[::-1]\n",
    "vt1new_ = vt1new_[::-1]\n",
    "vt0new = tf.stack(vt0new_,axis=2)\n",
    "vt1new = tf.stack(vt1new_,axis=2)\n",
    "\n",
    "\n",
    "step = tf.group(\n",
    "  vt0.assign(vt0new),\n",
    "  vt1.assign(vt1new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for i in range(20):\n",
    "        step.run()\n",
    "        Idnp = It[-1].eval()\n",
    "        ax[0].imshow(Idnp,cmap='gray',aspect='equal',interpolation='none')\n",
    "        ax[0].set_title('deformed image, iter {}'.format(i))\n",
    "        lambda1np = lambda1.eval()\n",
    "        ax[1].imshow(lambda1np,cmap='gray',aspect='equal',interpolation='none')\n",
    "        ax[1].set_title('error')\n",
    "        f.canvas.draw()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do\n",
    "I think more and more variables will keep getting added to the graph as we iterate.  I will want to use get_variable more carefully\n",
    "\n",
    "Well I'm not sure this is something I need to worry about.  the variables only get created once.  Then the graph updates itself.\n",
    "\n",
    "So what I need to do is fix boundary conditions on the gradient.\n",
    "\n",
    "\n",
    "And write the code in a nice way\n",
    "\n",
    "And move to 3D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
